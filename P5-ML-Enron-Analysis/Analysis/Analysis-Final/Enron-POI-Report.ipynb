{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron POI Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "<span style=\"color:purple\">_Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those? _ </style>\n",
    "___\n",
    "\n",
    "### Overview\n",
    "This project provided partially conclusive findings of suspected Enron employees in the 2002 Enron fraudlent scenario, **Person of Interest(POI)**, by using machine learning techniques.\n",
    "\n",
    "> We define POI  as someone who was indictedor settled in the case without admitting guilt. We utilized several techniques to come closer to answering the previously mentioned goal.\n",
    "\n",
    "### The Data \n",
    "\n",
    "The dataset we operated on was JSON formatted information on Enron employee financial statuses and email interactions. These features were integral in answering our question from the following reasoning:\n",
    "\n",
    "- Financial information is a central theme around industry corruption. For example, [Martin Shkreli heavily increased the price of a drug from $/$ $13.50 to $/$ $750.00.](https://www.scientificamerican.com/article/martin-shkreli-who-raised-drug-prices-from-13-50-to-750-arrested-in-securities-fraud-probe/) This action was from the fact that several drug companies implement the same methodology for profitability.\n",
    "\n",
    "- Communicating with a corrupt individual quite often is an indicator of aiding in abetting. \n",
    "\n",
    "To solve for POI concerns, we turned to machine learning algorithms for identifying corruption with our financial and email information. A classical approach in machine learning is e-mail classification from logistic regression within supervised learning. However, we implemented two other supervised learning algorithms to determine Enron POI.\n",
    "\n",
    "However, to implement these algorithms, we inspected the data for:\n",
    "\n",
    "1. Misinformation (typos)\n",
    "\n",
    "2. Lack of information (Null values)\n",
    "\n",
    "3. Obscure information (outliers)\n",
    "\n",
    "### Handling Outliers and more\n",
    "\n",
    "Using the inter-quartile range, we removed obscure data from our analysis and imputated some information. \n",
    "\n",
    "> We removed outlier entries such as \"TOTAL\" and two other entries due to \"NaN\" or absurd information recorded. This removal of infectious data enabled us to proceed with the analysis in better fashion than before.\n",
    "\n",
    "In the case of removing outliers, I just set some condition for extracting/neglecting outlier information as we update the dataset.\n",
    "\n",
    "In the case of imputing data, I created a function to replace existing \"NaN\" values with the integer \"0.\" Thereafter, we verified the changes in our introductory data analysis of Salary versus Bonus.\n",
    "___\n",
    "\n",
    "## Question 2\n",
    "\n",
    "<span style=\"color:purple\">_What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values._</span>\n",
    "\n",
    "We select several numerical features. Moreoever, we factor in the four engineered features, seen in the following subsection\n",
    "\n",
    "> We will implement four new features to potentially indentify Eron POI. \n",
    "\n",
    "> The **first two** additions are the ratio between an employee and the CEO, _Jeffrey K. Skilling_.\n",
    "\n",
    "> The implementation for these four new features can lead to the following:\n",
    "\n",
    "> - If the ratio in bonus between an employee and the CEO is significantly closer to the value 1, then we can suspect some type of corruption occuring.\n",
    "\n",
    "> - If the ratio in salary between an employee and the CEO is significantly closer to the value 1, then we can suspect some type of corruption occuring.\n",
    "\n",
    "> These features are linear transformations of the salary and bonus information presented. We factor how close ones's financial information is with respect to the CEO to determine POI relationship.\n",
    "\n",
    "> The **remaining two** additional features are the inbound and outbound email ratios between a POI.\n",
    "\n",
    "\n",
    "> The implementation for these four new features can lead to the following:\n",
    "\n",
    "> - If the ratio $\\dfrac{\\text{Received emails from POI}}{\\text{All received Emails}}$ is closer to the value 1, we can suspect an individual is a POI.\n",
    "\n",
    "> - If the ratio $\\dfrac{\\text{Sent emails from POI}}{\\text{All Sent Emails}}$ is closer to the value 1, we can suspect an individual is a POI.\n",
    "\n",
    "We then implemented a few algorithms to decide on what top three to four features we should implement for predicting people of interest in the Enron email scandal.\n",
    "\n",
    "I.e. we zoom out to consider the following features, then zoom in to precise feature selections to predict Enron fraudsters.\n",
    "\n",
    "1. POI\n",
    "\n",
    "2. CEO to Employee Bonus Ratio\n",
    "\n",
    "3. Total Payments\n",
    "\n",
    "4. Exercised Stock Options\n",
    "\n",
    "5. CEO to Employee Salary Ratio\n",
    "\n",
    "6. Restricted Stock\n",
    "\n",
    "7. Shared Receipt with POI\n",
    "\n",
    "8. FROM POI to This Person\n",
    "\n",
    "9. From Messages\n",
    "\n",
    "10. From this Person to POI\n",
    "\n",
    "11. Ratio of Sent Messages to POI\n",
    "\n",
    "12. Ratio of Received Messages to POI\n",
    "\n",
    "13. Deferral Payments\n",
    "\n",
    "14. Loan Advances\n",
    "\n",
    "15. Restricted Stock Deferred\n",
    "                 \n",
    "16. Deferred Income\n",
    "\n",
    "17. Expenses\n",
    "\n",
    "18. Other\n",
    "\n",
    "19. Long Term Incentive\n",
    "\n",
    "20. Director Fees\n",
    "\n",
    "> <a style = \"color:red\">Note:</a> **We do not** implement Salary and Bonus into our feature selection process due to the multicollinearity association to CEO to Employee Salary Ratio and CEO to Employee Bonus Ratio, respectively. Moreover, \"From Poi to this Person,\" \"From Messages\", and \"From this Person to Poi\" are collinear with ratio of interactions with POI\n",
    "\n",
    "> **Cutoff Criteria:** We select the top features of each algorithm implementation with a cutoff score being a a 5 digit difference between two sequential features (e.g. Total expenses being .10 and the next being 0.05) [SelectK best case: 5.0+ digit difference.]. \n",
    "\n",
    "Thereafter, we proceeeded through three iterations of a _feature selection_ process, where each case utilized a different algorithm. \n",
    "\n",
    "\n",
    "**Feature Selection with SKBest Procedure**\n",
    "\n",
    "We implement the [SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) method for selecting features according to the **k** highest scores. These k factors will be one of our few considerations for feature selection.\n",
    "\n",
    "> Added: Per correction/comment, 'k' looks at the features after score computation and keeps the top features for next fitting process.\n",
    "\n",
    "The following are the ranked features with the parameter **k**='all':\n",
    "\n",
    "![SelectKBest Ranking Features](../../Images/RankedFeat_SelectKBest.jpg)\n",
    "\n",
    "we observe that CEO to Employee Bonus Ratio and CEO to Employee Salary Ratio ranked within top 5 features for our model selection, as seen below:\n",
    "\n",
    "1. Exercised Stock Options\n",
    "\n",
    "2. Total Stock Value\n",
    "\n",
    "3. CEO To Employee Salary Ratio\n",
    "\n",
    "4. CEO To Employee Bonus Ratio\n",
    "\n",
    "5. Total Payments\n",
    "\n",
    "**Feature Selection with Extra Trees Implementation**\n",
    "\n",
    "\n",
    "The Extra Trees classifier is a variant of the popular Random Forest algorithm. However, each step of the Extra Trees implementation has random decision boundaries selected, rather than the best one. Moreover, Extra Trees classifier is great for our numerical features.\n",
    "\n",
    "The following are the ranked features with no altered parameters, default parameters.\n",
    "\n",
    "![Extra Trees Ranking Features](../../Images/RankedFeat_ExtraTrees.jpg)\n",
    "\n",
    "Our findings within the Extra Trees implementation had Exercised Stock Options  as our top feature recommendation.\n",
    "\n",
    "\n",
    "**Feature Selection with Decision Tree Algorithm**\n",
    "\n",
    "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.\n",
    "\n",
    "The following are the ranked features with no altered parameters, default parameters.\n",
    "\n",
    "![Extra Trees Ranking Features](../../Images/RankedFeat_DT.jpg)\n",
    "\n",
    "For ranking the importance of all recommended features, The top important features are:\n",
    "\n",
    "1. Total Stock Value\n",
    "\n",
    "2. CEO to Employee Bonus Ratio\n",
    "\n",
    "3. Expenses\n",
    "\n",
    "Our top features only have one commmon feature in common from the results of our other features. This feature is CEO to Employee Bonus Ratio.\n",
    "\n",
    "Moreover, this scenario has a precision of 0.20 and recall of 0.40 for determining POI. This is partially good to see, however we should observe that this, and the past outcomes, is not optimal for model implementation. I.e.,Notice that we implemented the Decision Tree algorithm with default parameters, and previous algorithms as well. This consideration occurs because we do not know if our feature selection process was optimal in selection. \n",
    "\n",
    "### Feature Selection: Final Decision\n",
    "\n",
    "\n",
    "The top re-occuring features to select from where, in frequency:\n",
    "\n",
    "1. CEO To Employee Bonus Ratio\n",
    "\n",
    "2. CEO To Employee Salary Ratio\n",
    "\n",
    "3. Exercised Stock Options\n",
    "\n",
    "Additionally, we include two additionaly engineered features:\n",
    "\n",
    "4. ratio_of_received_messages_to_poi\n",
    "\n",
    "5. ratio_of_sent_messages_to_poi\n",
    "\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "## Question 3\n",
    "\n",
    "<span style=\"color:purple\">_What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?_</span>\n",
    "\n",
    "We implemented SelectKbest, Extra Trees, and Decision Tree algorithm for obtaining an optimal feature count for model fitting and ranking features to best classify POI of Enron employees.\n",
    "\n",
    "\n",
    "Thereafter, we compared and contrasted between Gaussian Naive Baye's algorithm and Decision Tree implementions to best predict POI.\n",
    "\n",
    "The following are the results from the Gaussian Naive Baye's implementation:\n",
    "\n",
    "![Gaussian Naive Baye's Results](../../Images/Tuning_GNB1.jpg)\n",
    "\n",
    "![Gaussian Naive Baye's Results](../../Images/Tuning_GNB2.jpg)\n",
    "\n",
    "\n",
    "This process took 8.8seconds from a total of 1600 fits. We had a recommendation of priors being \"None\" for the Naive Baye's Implementation. The resulting model performance from this recommendation of hypertuning was a precision of 33%, recall of 20%, and an accuracy of ~84%. \n",
    "\n",
    "This is somewhat acceptable. we now turn to the Decision Tree implemenation to hopefully obtain more desirable results.\n",
    "\n",
    "The following are the results from the Decision Tree's implementation:\n",
    "\n",
    "![Gaussian Naive Baye's Results](../../Images/Tuning_DT1.jpg)\n",
    "\n",
    "![Gaussian Naive Baye's Results](../../Images/Tuning_DT3.jpg)\n",
    "\n",
    "This process took 1.8minutes from a total of 16200 fits. We had parameter recommendations of: \n",
    "\n",
    "1. class_weight=None\n",
    "2. criterion='gini'\n",
    "3. max_depth=5,\n",
    "4. max_features=None\n",
    "5. max_leaf_nodes=None\n",
    "6. min_impurity_split=1e-07\n",
    "7. min_samples_leaf=7\n",
    "8. min_samples_split=8\n",
    "9. min_weight_fraction_leaf=0.04\n",
    "10. splitter='best'\n",
    "\n",
    "for the Naive Baye's Implementation. \n",
    "\n",
    "The resulting model performance from this recommendation of hypertuning was a precision of 67%, recall of 40%, and an accuracy of ~89.74%. \n",
    "\n",
    "Though we sacrificed time in the Decision Tree implementation, we obtained better results than that of the Gaussian Naive Baye's algorithm. Therefore, we proceeded into our analysis with the Decision Tree implementation.\n",
    "\n",
    "___\n",
    "\n",
    "## Question 4\n",
    "\n",
    "<span style=\"color:purple\">_What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune?_</span>\n",
    "\n",
    "Tuning parameters for an algorithm is a rigorous attempt at finding some optimal factors/identifiers to best create a statistical forecast model. If you we failed to reject or even disvalue the importance of tuning, we fail to find the a both precise and accurate model for future cases outside some past observations.\n",
    "\n",
    "We obtained several model performance results from Gaussian Baive Baye's Implementation in the following:\n",
    "\n",
    "| Metrics |  Gaussian Naive Bayes Algorithm Time before Tuning(seconds)  |Accuracy before Tuning|Precision Before Tuning|Recall before Tuning|F1-Score before Tuning|Gaussian Naive Bayes Algorithm Time after Tuning (seconds) |Accuracy after Tuning|Precision after Tuning|Recall after Tuning|F1-Score after Tuning|\n",
    "|--------|--------|--------|--------|--------|--------|--------|--------|------|------|----|\n",
    "|  Default Settings | 0.005| 0.846| 0.33| 0.20| 0.25| **0.005**| **0.846**| **0.33**| **0.20**| **0.25**|\n",
    "|Non-POI 90% | 0.005| 0.846| 0.33| 0.20| 0.25| **0.002**| **0.128**|**0.13**| **1.00**| **0.23**|\n",
    "|Non-POI 60%| 0.005| 0.846| 0.33| 0.20| 0.25| **0.001**| **0.795**| **0.20**| **0.20**| **0.20**|\n",
    "|Non-POI 10% | 0.005| 0.846| 0.33| 0.20| 0.25| **0.001**| **0.846**|** .33**| **0.20 **|**0.25**|\n",
    "\n",
    "We observe the Gaussian Naive Bayes implementation is best with default settings, or prior probability of Non-POI at/around 10%.\n",
    "\n",
    "\n",
    "We utilized the Decision Tree algorithm. Other algorithms utilized were Extra Trees and Select K Best. The perfomance from each model were quick. However, the results were inconsistent and mixed up. Moreover, there were faults to each scenario. However, the Decision Tree worked more favorable with obtaining optimal features for our logistic problem.\n",
    "\n",
    "The following is outputs for Decision Tree Implementation with manual tuning:\n",
    "\n",
    "| Metrics |  Decision Tree Algorithm Time before Tuning(seconds)  |Accuracy before Tuning|Precision Before Tuning|Recall before Tuning|F1-Score before Tuning| Decision Tree Algorithm Time after Tuning (seconds) |Accuracy after Tuning|Precision after Tuning|Recall after Tuning|F1-Score after Tuning|\n",
    "|--------|--------|--------|--------|--------|--------|--------|--------|------|------|----|\n",
    "|  Default Settings | 0.003| 0.821| 0.33| 0.40| 0.36| **0.003**| **0.821**| **0.33**| **0.40**| **0.36**|\n",
    "|min_weight_fraction_leaf=0.0001 | 0.003| 0.821| 0.33| 0.40| 0.36| **0.002**| **0.821**|**0.33**| **0.40**| **0.36**|\n",
    "|min_samples_split=3| 0.003| 0.821| 0.33| 0.40| 0.36| **0.001**| **0.8210**| **0.33**| **0.40**| **0.36**|\n",
    "|min_samples_leaf=5 | 0.003| 0.821| 0.33| 0.40| 0.36| **0.002**| **0.8462**|** .33**| **0.20 **|**0.25**|\n",
    "|max_depth = 7 | 0.003| 0.821| 0.33| 0.40| 0.36| **0.001**| **0.821**| **0.33**| **0.40**| **0.36**|\n",
    "\n",
    "Everything highlighted in bold, the latter five columns are different parameters being tuned. The firt five columns are the default Decision Tree Algorithm model output.\n",
    "\n",
    "Recalling that our Decision Tree implementation was a more ideal approach for model optimization, we utilized the Decision Tree implementation we tuned the following parameters:\n",
    "\n",
    "1. max_depth\n",
    "\n",
    "2. max_leaf_nodes\n",
    "\n",
    "3. min_samples_leaf\n",
    "\n",
    "4. min_samples_split\n",
    "\n",
    "5. min_weight_fraction_leaf\n",
    "\n",
    "\n",
    "We selected these factors based on computation and processing speed. If we had a better computing system (#IWishIhadCloudComputing) our tuning could have been more broad and deep.\n",
    "\n",
    "The following details is our implemenation of hypertuning with Pipeline and GridSearchCV implemenation.\n",
    "\n",
    "From the tester tuning these parameters, they can\n",
    "1. Evaluated our data faster\n",
    "2. Confirmed Optimal Accuracy\n",
    "\n",
    "Utilizing the Pipeline and GridSearchCV libraries, the following code provided\n",
    "\n",
    "fitting 1000 folds for each of 432 candidates, totalling 432000 fits, as seen below\n",
    "\n",
    "> Pipeline(steps=[('kbest', SelectKBest(k='all', score_func=\\<function f_classif at 0x00000000099DB518>)), \n",
    "\n",
    "> ('dtree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
    "\n",
    "> max_features=None, max_leaf_nodes=None,\n",
    "\n",
    "> min_impurity_split=1e-07, min_samples_leaf=5,\n",
    "\n",
    "> min_samples_split=7, min_weight_fraction_leaf=0.001,\n",
    "\n",
    "> presort=False, random_state=None, splitter='best'))])\n",
    "\n",
    "Again we see, utilizing the recommended parameters and feature_list features that we manually selected, we receive the following output from tester.py:\n",
    "\n",
    "![Tester python file](../../Images/tester.jpg)\n",
    "\n",
    "We have recall and precision scores of .30+. Moreover, our accuracy is ~0.86%!\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "## Question 5\n",
    "\n",
    "<span style=\"color:purple\">What is validation, and what’s a classic mistake you can make if you do it wrong?</span>\n",
    "\n",
    "Validation is referred to as the process where a trained model is evaluated with a testing data set. With this partitioning of data, validation serves a purpose of using the testing data to test a trained model for generalizations. We test our trained model's precision, accuracy, and recall rates. \n",
    "\n",
    "One classic mistake if you can do it wrong is incorrectly guessing future cases in which affect production/company performance. This incorrect predictions are commonly due to overfitting or underfitting the model shaped by the training data.\n",
    "\n",
    "> A brief analogy of improper fitting for predictions can be guessing your time of arrival of meeting a friend. You usually take a path for 15minutes, and therefore tell this person. However, you take the busy street adjacent to the path you usually take, and become 3minutes late, due to traffice. The model was predicting your time of arrival from inputs like exprience crossing street and pace you travel. This is an example of **overfitting.**\n",
    "\n",
    "## Question 6 \n",
    "\n",
    "<span style=\"color:purple\">_Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]_</span>\n",
    "\n",
    "\n",
    "\n",
    "Fitting 100 folds for each of 162 candidates, totalling 16200 fits, as seen below\n",
    "\n",
    "> Pipeline(steps=[('kbest', SelectKBest(k='all', score_func=\\<function f_classif at 0x00000000099DB518>)), \n",
    "\n",
    "> ('dtree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
    "\n",
    "> max_features=None, max_leaf_nodes=None,\n",
    "\n",
    "> min_impurity_split=1e-07, min_samples_leaf=5,\n",
    "\n",
    "> min_samples_split=7, min_weight_fraction_leaf=0.001,\n",
    "\n",
    "> presort=False, random_state=None, splitter='best'))])\n",
    "\n",
    "Utilizing the recommended parameters and feature_list features that we manually selected, we receive the following output from tester.py:\n",
    "\n",
    "![Tester python file](../../Images/tester.jpg)\n",
    "\n",
    "We have recall and precision scores of .30+. Moreover, our accuracy is ~0.86!\n",
    "\n",
    "> Note: When TP < FP, then accuracy will always increase when we change a classification rule to always output “negative” category. Conversely, when TN < FN, the same will happen when we change our rule to always output “positive\n",
    "> This following section also provides the Precision, Recall, and F1-Score related to our implemented models. \n",
    "\n",
    "> In our case,\n",
    "\n",
    ">**Precision** (TP)/(TP+FP) cares about whether the positive examples predicted by our model were correct. In our case, what's the % Enron employees classified as POI correctly out of all classified Enron Employees classified as POI.\n",
    "\n",
    ">**Recall** (TP)/(TP+FN) cares more on whether we have predicted all positive examples in the data. In our case, what is the percent of predictions were correctly identified POI, for all actual POI.\n",
    "\n",
    "\n",
    ">where TP:=True Postive, FN:=False Negative, FP:= False Postives, TN:= True Negatives, as seen below\n",
    "\n",
    "\n",
    "\n",
    "| True State/Diagnosis | NOT POI | POI |\n",
    "|---------------------:|---------|-----|\n",
    "|              NOT POI | TN      | FP  |\n",
    "|                  POI | FN      | TP  |\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "## Remarks\n",
    "\n",
    "**Quick Recap**\n",
    "\n",
    "- We dealt with with an imperfect, real-world dataset\n",
    "\n",
    "- We validated a machine learning result using test data\n",
    "\n",
    "- We evaluated a machine learning result using quantitative metrics\n",
    "\n",
    "- We created, select and transform features\n",
    "\n",
    "- We hypter tuned machine learning algorithms for maximum performance\n",
    "\n",
    "**Interesting Fact**\n",
    "\n",
    "- Testing data is a high variance outcome. In future cases, we should be concerned with overfitting or underfitting, for the sake of abiding to the variance contraint from testing data.\n",
    "\n",
    "\n",
    "**Comparing Cross-Validation with Train/Test splits:**\n",
    "\n",
    "- Cross Validation:\n",
    "\n",
    "     + More accurate estimate for out-of-sample accuracy\n",
    "\n",
    "    + more efficient use of data\n",
    "    \n",
    "- Train/Test Split\n",
    "    + Runs K times faster than K-fold cross validation\n",
    "    + Simplier to exampled detailed results\n",
    "    \n",
    "**Feature Engineering**\n",
    "\n",
    "- Our CEO Salary Bonus/Salary Ratios were collinear with Bonus and Salary, respectively. Therefore, we removed Bonus and Salary as feature considerations for our analysis. If we would have implemented these features, accuracy and other performance factors for predicting POI Enron Employees would have been heavily inflated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "1. [What is Pickling in Python?](https://pythontips.com/2013/08/02/what-is-pickle-in-python/)\n",
    "\n",
    "    a. [Video on Pickling](https://www.youtube.com/watch?v=2Tw39kZIbhs)\n",
    "    \n",
    "2. [Recursive Feature Elminations](http://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "\n",
    "3. [General feature selection in cross-validation](https://www.youtube.com/watch?v=6dbrR-WymjI)\n",
    "\n",
    "4. [Hypertuning](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning))\n",
    "\n",
    "5. [SKBest and GridSearch Information](https://www.civisanalytics.com/blog/workflows-in-python-using-pipeline-and-gridsearchcv-for-more-compact-and-comprehensive-code/)\n",
    "\n",
    "6. [SVM versus Decision Trees](https://stats.stackexchange.com/questions/57438/why-is-svm-not-so-good-as-decision-tree-on-the-same-data)\n",
    "\n",
    "7. [Model Metrics and Ranking](http://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf)\n",
    "\n",
    "8. [Class Imbalance Problem](http://www.chioka.in/class-imbalance-problem/)\n",
    "\n",
    "9. [F1-Score](https://datascience.stackexchange.com/questions/11014/why-are-precision-and-recall-used-in-the-f1-score-rather-than-precision-and-npv)\n",
    "\n",
    "10. [Precision vs Recall](https://www.quora.com/What-is-the-best-way-to-understand-the-terms-precision-and-recall)\n",
    "    \n",
    "    a. [Precision vs Recall Blog](http://rushdishams.blogspot.co.id/2011/03/precision-and-recall.html)\n",
    "11. [Accuracy versus Precision, Recall, and F1-Score](https://tryolabs.com/blog/2013/03/25/why-accuracy-alone-bad-measure-classification-tasks-and-what-we-can-do-about-it/)\n",
    "\n",
    "12. [Multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)\n",
    "\n",
    "13. [Stratification in Train and Test data](https://stackoverflow.com/questions/34842405/parameter-stratify-from-method-train-test-split-scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary \n",
    "\n",
    "1. **Financial Features (units in US Dollars):**\n",
    "    \n",
    "    a. **Salary:** Reflects items such as base salary, executive cash allowances, and benefits payments.\n",
    "    \n",
    "    b. **Deferral Payments:** Reflects distributions from a deferred compensation arrangement due to termination of employment or due to in-service withdrawals as per plan provisions.\n",
    "    \n",
    "    c. **Total Payments:**\n",
    "    \n",
    "    d. **Loan Advances:** Reflects total amount of loan advances, excluding repayments, provided by the Debtor in return for a promise of repayment. In certain instances, the terms of the promissory notes allow for the option to repay with stock of the company.\n",
    "    \n",
    "    e. **Bonus:** Reflects annual cash incentives paid based upon company performance. Also may include other retention payments.\n",
    "    \n",
    "    f. **Restricted Stock Deferred:** Reflects value of restricted stock voluntarily deferred prior to release under a deferred compensation arranged\n",
    "    \n",
    "    g. **Deferred Income:** Reflects voluntary executive deferrals of salary, annual cash incentives, and long-term cash incentives as well as cash fees deferred by non-employee directors under a deferred compensation arrangement. May also reflect deferrals under a stock option or phantom stock unit in lieu of cash arrangement.\n",
    "    \n",
    "    h. **Total Stock Value:** In 1998, 1999 and 2000, Debtor and non-debtor affiliates were charged for options granted. The Black-Scholes method was used to determine the amount to be charged. Any amounts charged to Debtor and non-debtor affiliates associated with the options exercised related to these three years have not been subtracted from the share value amounts shown.\n",
    "    \n",
    "    i. **Expenses:** Reflects reimbursements of business expenses. May include fees paid for consulting services.\n",
    "    \n",
    "    j. **Exercised Stock Options:** Reflects amounts from exercised stock options which equal the market value in excess of the exercise price on the date the options were exercised either through cashless (same-day sale), stock swap or cash exercises. The reflected gain may differ from that realized by the insider due to fluctuations in the market price and the timing of any subsequent sale of the securities.\n",
    "    \n",
    "    k. **Other:** Reflects items such as payments for severance, consulting services, relocation costs, tax advances and allowances for employees on international assignment\n",
    "    \n",
    "    l. **Long Term Incentive:** Reflects long-term incentive cash payments from various long-term incentive programs designed to tie executive compensation to long-term success as measured against key performance drivers and business objectives over a multi-year period, generally 3 to 5 years.\n",
    "    \n",
    "    m. **Restricted Stock:** Reflects the gross fair market value of shares and accrued dividends (and/or phantom units and dividend equivalents) on the date of release due to lapse of vesting periods, regardless of whether deferred.\n",
    "    \n",
    "    n. **Director Fees:** Reflects cash payments and/or value of stock grants made in lieu of cash payments to non-employee directors.\n",
    "2. **Email Features:**\n",
    "\n",
    "    a. **To Messages**\n",
    "    \n",
    "    b. **Email Address**\n",
    "    \n",
    "    c. **From POI to this Person**\n",
    "    \n",
    "    d. **From Messages**\n",
    "    \n",
    "    e. **From This Person To POI**\n",
    "    \n",
    "    f. **Shared Receipt With POI**\n",
    "3. **POI label:**\n",
    "\n",
    "    a. **POI (person of interest):** Person suspected in fraudulent actions during 2002 Enron Crisis"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python27]",
   "language": "python",
   "name": "conda-env-python27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
