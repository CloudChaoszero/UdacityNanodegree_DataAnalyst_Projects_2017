{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron POI Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "<span style=\"color:purple\">_Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those? _ </style>\n",
    "___\n",
    "\n",
    "### Overview\n",
    "This project provided partially conclusive findings of suspected Enron employees in the 2002 Enron fraudlent scenario, **Person of Interest(POI)**, by using machine learning techniques.\n",
    "\n",
    "> We define POI  as someone who was indictedor settled in the case without admitting guilt. We utilized several techniques to come closer to answering the previously mentioned goal.\n",
    "\n",
    "### The Data \n",
    "\n",
    "The dataset we operated on was JSON formatted information on Enron employee financial statuses and email interactions. These features were integral in answering our question from the following reasoning:\n",
    "\n",
    "- Financial information is a central theme around industry corruption.\n",
    "\n",
    "- Communicating with a corrupt individual quite often is an indicator of aiding in abetting. \n",
    "\n",
    "To solve for POI concerns, we turned to machine learning algorithms for identifying corruption with our financial and email information. A classical approach in machine learning is e-mail classification from logistic regression within supervised learning. However, we implemented two other supervised learning algorithms to determine Enron POI.\n",
    "\n",
    "However, to implement these algorithms, we inspected the data for:\n",
    "\n",
    "1. Misinformation (typos)\n",
    "\n",
    "2. Lack of information (Null values)\n",
    "\n",
    "3. Obscure information (outliers)\n",
    "\n",
    "### Handling Outliers and more\n",
    "\n",
    "Using the inter-quartile range, we removed obscure data from our analysis and imputated some information. \n",
    "\n",
    "> We removed outlier entries such as \"TOTAL\" and two other entries due to \"NaN\" or absurd information recorded. This removal of infectious data enabled us to proceed with the analysis in better fashion than before.\n",
    "\n",
    "In the case of removing outliers, I just set some condition for extracting/neglecting outlier information as we update the dataset.\n",
    "\n",
    "In the case of imputing data, I created a function to replace existing \"NaN\" values with the integer \"0.\" Thereafter, we verified the changes in our introductory data analysis of Salary versus Bonus.\n",
    "___\n",
    "\n",
    "## Question 2\n",
    "\n",
    "<span style=\"color:purple\">_What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values._</span>\n",
    "\n",
    "We select several numerical features. Moreoever, we factor in the four engineered features, seen in the following subsection\n",
    "\n",
    "> We will implement four new features to potentially indentify Eron POI. \n",
    "\n",
    "> The **first two** additions are the ratio between an employee and the CEO, _Jeffrey K. Skilling_.\n",
    "\n",
    "> The implementation for these four new features can lead to the following:\n",
    "\n",
    "> - If the ratio in bonus between an employee and the CEO is significantly closer to the value 1, then we can suspect some type of corruption occuring.\n",
    "\n",
    "> - If the ratio in salary between an employee and the CEO is significantly closer to the value 1, then we can suspect some type of corruption occuring.\n",
    "\n",
    "> These features are linear transformations of the salary and bonus information presented. We factor how close ones's financial information is with respect to the CEO to determine POI relationship.\n",
    "\n",
    "> The **remaining two** additional features are the inbound and outbound email ratios between a POI.\n",
    "\n",
    "\n",
    "> The implementation for these four new features can lead to the following:\n",
    "\n",
    "> - If the ratio $\\dfrac{\\text{Received emails from POI}}{\\text{All received Emails}}$ is closer to the value 1, we can suspect an individual is a POI.\n",
    "\n",
    "> - If the ratio $\\dfrac{\\text{Sent emails from POI}}{\\text{All Sent Emails}}$ is closer to the value 1, we can suspect an individual is a POI.\n",
    "\n",
    "We then implemented a few algorithms to decide on what top three to four features we should implement for predicting people of interest in the Enron email scandal.\n",
    "\n",
    "I.e. we zoom out to consider the following features, then zoom in to precise feature selections to predict Enron fraudsters.\n",
    "\n",
    "1. POI\n",
    "\n",
    "2. CEO to Employee Bonus Ratio\n",
    "\n",
    "3. Total Payments\n",
    "\n",
    "4. Exercised Stock Options\n",
    "\n",
    "5. CEO to Employee Salary Ratio\n",
    "\n",
    "6. Restricted Stock\n",
    "\n",
    "7. Shared Receipt with POI\n",
    "\n",
    "8. FROM POI to This Person\n",
    "\n",
    "9. From Messages\n",
    "\n",
    "10. From this Person to POI\n",
    "\n",
    "11. Ratio of Sent Messages to POI\n",
    "\n",
    "12. Ratio of Received Messages to POI\n",
    "\n",
    "13. Deferral Payments\n",
    "\n",
    "14. Loan Advances\n",
    "\n",
    "15. Restricted Stock Deferred\n",
    "                 \n",
    "16. Deferred Income\n",
    "\n",
    "17. Expenses\n",
    "\n",
    "18. Other\n",
    "\n",
    "19. Long Term Incentive\n",
    "\n",
    "20. Director Fees\n",
    "\n",
    "\n",
    "\n",
    "**Feature Selection with SKBest Procedure**\n",
    "\n",
    "We implement the [SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) method for selecting features according to the **k** highest scores. These k factors will be one of our few considerations for feature selection.\n",
    "\n",
    "> Added: Per correction/comment, 'k' looks at the features after score computation and keeps the top features for next fitting process.\n",
    "\n",
    "The following are the ranked features with the parameter **k**='all':\n",
    "\n",
    "![SelectKBest Ranking Features](../../Images/RankedFeat_SelectKBest.jpg)\n",
    "\n",
    "we observe that CEO to Employee Bonus Ratio and CEO to Employee Salary Ratio ranked within top 5 features for our model selection, as seen below:\n",
    "\n",
    "1. Exercised Stock Options\n",
    "\n",
    "2. Total Stock Value\n",
    "\n",
    "3. CEO To Employee Salary Ratio\n",
    "\n",
    "4. CEO To Employee Bonus Ratio\n",
    "\n",
    "5. Total Payments\n",
    "\n",
    "**Feature Selection with Extra Trees Implementation**\n",
    "\n",
    "\n",
    "The Extra Trees classifier is a variant of the popular Random Forest algorithm. However, each step of the Extra Trees implementation has random decision boundaries selected, rather than the best one. Moreover, Extra Trees classifier is great for our numerical features.\n",
    "\n",
    "The following are the ranked features with no altered parameters, default parameters.\n",
    "\n",
    "![Extra Trees Ranking Features](../../Images/RankedFeat_ExtraTrees.jpg)\n",
    "\n",
    "Our findings within the Extra Trees implementation had Exercised Stock Options  as our top feature recommendation.\n",
    "\n",
    "\n",
    "**Feature Selection with Decision Tree Algorithm**\n",
    "\n",
    "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.\n",
    "\n",
    "The following are the ranked features with no altered parameters, default parameters.\n",
    "\n",
    "![Extra Trees Ranking Features](../../Images/RankedFeat_DT.jpg)\n",
    "\n",
    "For ranking the importance of all recommended features, The top important features are:\n",
    "\n",
    "1. Total Stock Value\n",
    "\n",
    "2. CEO to Employee Bonus Ratio\n",
    "\n",
    "3. Expenses\n",
    "\n",
    "Our top features only have one commmon feature in common from the results of our other features. This feature is CEO to Employee Bonus Ratio.\n",
    "\n",
    "Moreover, this scenario has a precision of 0.20 and recall of 0.40 for determining POI. This is partially good to see, however we should observe that this, and the past outcomes, is not optimal for model implementation. I.e.,Notice that we implemented the Decision Tree algorithm with default parameters, and previous algorithms as well. This consideration occurs because we do not know if our feature selection process was optimal in selection. \n",
    "\n",
    "### Feature Selection: Final Decision\n",
    "\n",
    "\n",
    "The top re-occuring features to select from where, in frequency:\n",
    "\n",
    "1. CEO To Employee Bonus Ratio\n",
    "\n",
    "2. CEO To Employee Salary Ratio\n",
    "\n",
    "3. Exercised Stock Options\n",
    "\n",
    "Additionally, we include two additionaly engineered features:\n",
    "\n",
    "4. ratio_of_received_messages_to_poi\n",
    "\n",
    "5. ratio_of_sent_messages_to_poi\n",
    "\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "## Question 3\n",
    "\n",
    "<span style=\"color:purple\">_What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?_</span>\n",
    "\n",
    "We implemented SelectKbest, Extra Trees, and Decision Tree algorithm for obtaining an optimal feature count for model fitting and ranking features to best classify POI of Enron employees.\n",
    "\n",
    "\n",
    "Thereafter, we compared and contrasted between Gaussian Naive Baye's algorithm and Decision Tree implementions to best predict POI.\n",
    "\n",
    "The following are the results from the Gaussian Naive Baye's implementation:\n",
    "\n",
    "![Gaussian Naive Baye's Results](../../Images/Tuning_GNB1.jpg)\n",
    "\n",
    "![Gaussian Naive Baye's Results](../../Images/Tuning_GNB2.jpg)\n",
    "\n",
    "\n",
    "This process took 8.8seconds from a total of 1600 fits. We had a recommendation of priors being \"None\" for the Naive Baye's Implementation. The resulting model performance from this recommendation of hypertuning was a precision of 33%, recall of 20%, and an accuracy of ~84%. \n",
    "\n",
    "This is somewhat acceptable. we now turn to the Decision Tree implemenation to hopefully obtain more desirable results.\n",
    "\n",
    "The following are the results from the Decision Tree's implementation:\n",
    "\n",
    "![Gaussian Naive Baye's Results](../../Images/Tuning_DT1.jpg)\n",
    "\n",
    "![Gaussian Naive Baye's Results](../../Images/Tuning_DT3.jpg)\n",
    "\n",
    "This process took 1.8minutes from a total of 16200 fits. We had parameter recommendations of: \n",
    "\n",
    "1. class_weight=None\n",
    "2. criterion='gini'\n",
    "3. max_depth=5,\n",
    "4. max_features=None\n",
    "5. max_leaf_nodes=None\n",
    "6. min_impurity_split=1e-07\n",
    "7. min_samples_leaf=7\n",
    "8. min_samples_split=8\n",
    "9. min_weight_fraction_leaf=0.04\n",
    "10. splitter='best'\n",
    "\n",
    "for the Naive Baye's Implementation. \n",
    "\n",
    "The resulting model performance from this recommendation of hypertuning was a precision of 67%, recall of 40%, and an accuracy of ~89.74%. \n",
    "\n",
    "Though we sacrificed time in the Decision Tree implementation, we obtained better results than that of the Gaussian Naive Baye's algorithm. Therefore, we proceeded into our analysis with the Decision Tree implementation.\n",
    "\n",
    "___\n",
    "\n",
    "## Question 4\n",
    "\n",
    "<span style=\"color:purple\">_What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune?_</span>\n",
    "\n",
    "Tuning parameters for an algorithm is a rigorous attempt at finding some optimal factors/identifiers to best create a statistical forecast model. If you we failed to reject or even disvalue the importance of tuning, we fail to find the a both precise and accurate model for future cases outside some past observations.\n",
    "\n",
    "We obtained several model performance results from Gaussian Baive Baye's Implementation in the following:\n",
    "\n",
    "| Metrics |  Gaussian Naive Bayes Algorithm Time before Tuning(seconds)  |Accuracy before Tuning|Precision Before Tuning|Recall before Tuning|F1-Score before Tuning|Gaussian Naive Bayes Algorithm Time after Tuning (seconds) |Accuracy after Tuning|Precision after Tuning|Recall after Tuning|F1-Score after Tuning|\n",
    "|--------|--------|--------|--------|--------|--------|--------|--------|------|------|----|\n",
    "|  Default Settings | 0.005| 0.846| 0.33| 0.20| 0.25| **0.005**| **0.846**| **0.33**| **0.20**| **0.25**|\n",
    "|Non-POI 90% | 0.005| 0.846| 0.33| 0.20| 0.25| **0.002**| **0.128**|**0.13**| **1.00**| **0.23**|\n",
    "|Non-POI 60%| 0.005| 0.846| 0.33| 0.20| 0.25| **0.001**| **0.795**| **0.20**| **0.20**| **0.20**|\n",
    "|Non-POI 10% | 0.005| 0.846| 0.33| 0.20| 0.25| **0.001**| **0.846**|** .33**| **0.20 **|**0.25**|\n",
    "\n",
    "We observe the Gaussian Naive Bayes implementation is best with default settings, or prior probability of Non-POI at/around 10%.\n",
    "\n",
    "The following is outputs for Decision Tree Implementation with manual tuning:\n",
    "\n",
    "| Metrics |  Decision Tree Algorithm Time before Tuning(seconds)  |Accuracy before Tuning|Precision Before Tuning|Recall before Tuning|F1-Score before Tuning| Decision Tree Algorithm Time after Tuning (seconds) |Accuracy after Tuning|Precision after Tuning|Recall after Tuning|F1-Score after Tuning|\n",
    "|--------|--------|--------|--------|--------|--------|--------|--------|------|------|----|\n",
    "|  Default Settings | 0.003| 0.821| 0.33| 0.40| 0.36| **0.003**| **0.821**| **0.33**| **0.40**| **0.36**|\n",
    "|min_weight_fraction_leaf=0.0001 | 0.003| 0.821| 0.33| 0.40| 0.36| **0.002**| **0.821**|**0.33**| **0.40**| **0.36**|\n",
    "|min_samples_split=3| 0.003| 0.821| 0.33| 0.40| 0.36| **0.001**| **0.8210**| **0.33**| **0.40**| **0.36**|\n",
    "|min_samples_leaf=5 | 0.003| 0.821| 0.33| 0.40| 0.36| **0.002**| **0.8462**|** .33**| **0.20 **|**0.25**|\n",
    "|max_depth = 7 | 0.003| 0.821| 0.33| 0.40| 0.36| **0.001**| **0.821**| **0.33**| **0.40**| **0.36**|\n",
    "\n",
    "Everything highlighted in bold, the latter five columns are different parameters being tuned. The firt five columns are the default Decision Tree Algorithm model output.\n",
    "\n",
    "\n",
    "Utilizing the Pipeline and GridSearchCV libraries, the following code provided\n",
    "\n",
    "fitting 1000 folds for each of 432 candidates, totalling 432000 fits, as seen below\n",
    "\n",
    "> Pipeline(steps=[('kbest', SelectKBest(k='all', score_func=\\<function f_classif at 0x00000000099DB518>)), \n",
    "\n",
    "> ('dtree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
    "\n",
    "> max_features=None, max_leaf_nodes=None,\n",
    "\n",
    "> min_impurity_split=1e-07, min_samples_leaf=5,\n",
    "\n",
    "> min_samples_split=7, min_weight_fraction_leaf=0.001,\n",
    "\n",
    "> presort=False, random_state=None, splitter='best'))])\n",
    "\n",
    "Again we see, utilizing the recommended parameters and feature_list features that we manually selected, we receive the following output from tester.py:\n",
    "\n",
    "![Tester python file](../../Images/tester.jpg)\n",
    "\n",
    "We have recall and precision scores of .30+. Moreover, our accuracy is ~0.86%!\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "## Question 5\n",
    "\n",
    "<span style=\"color:purple\">What is validation, and what’s a classic mistake you can make if you do it wrong?</span>\n",
    "\n",
    "Validation is referred to as the process where a trained model is evaluated with a testing data set. With this partitioning of data, validation serves a purpose of using the testing data to test a trained model for generalizations. We test our trained model's precision, accuracy, and recall rates. \n",
    "\n",
    "One classic mistake if you can do it wrong is incorrectly guessing future cases in which affect production/company performance. This incorrect predictions are commonly due to overfitting or underfitting the model shaped by the training data.\n",
    "\n",
    "\n",
    "## Question 6 \n",
    "\n",
    "<span style=\"color:purple\">_Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]_</span>\n",
    "\n",
    "\n",
    "\n",
    "Fitting 100 folds for each of 162 candidates, totalling 16200 fits, as seen below\n",
    "\n",
    "> Pipeline(steps=[('kbest', SelectKBest(k='all', score_func=\\<function f_classif at 0x00000000099DB518>)), \n",
    "\n",
    "> ('dtree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
    "\n",
    "> max_features=None, max_leaf_nodes=None,\n",
    "\n",
    "> min_impurity_split=1e-07, min_samples_leaf=5,\n",
    "\n",
    "> min_samples_split=7, min_weight_fraction_leaf=0.001,\n",
    "\n",
    "> presort=False, random_state=None, splitter='best'))])\n",
    "\n",
    "Utilizing the recommended parameters and feature_list features that we manually selected, we receive the following output from tester.py:\n",
    "\n",
    "![Tester python file](../../Images/tester.jpg)\n",
    "\n",
    "We have recall and precision scores of .30+. Moreover, our accuracy is ~0.86!\n",
    "\n",
    "> Note: When TP < FP, then accuracy will always increase when we change a classification rule to always output “negative” category. Conversely, when TN < FN, the same will happen when we change our rule to always output “positive\n",
    "> This following section also provides the Precision, Recall, and F1-Score related to our implemented models. \n",
    "\n",
    "> In our case,\n",
    "\n",
    ">**Precision** (TP)/(TP+FP) cares about whether the positive examples predicted by our model were correct. In our case, what's the % Enron employees classified as POI correctly out of all classified Enron Employees classified as POI.\n",
    "\n",
    ">**Recall** (TP)/(TP+FN) cares more on whether we have predicted all positive examples in the data. In our case, what is the percent of predictions were correctly identified POI, for all actual POI.\n",
    "\n",
    "\n",
    ">where TP:=True Postive, FN:=False Negative, FP:= False Postives, TN:= True Negatives, as seen below\n",
    "\n",
    "\n",
    "\n",
    "| True State/Diagnosis | NOT POI | POI |\n",
    "|---------------------:|---------|-----|\n",
    "|              NOT POI | TN      | FP  |\n",
    "|                  POI | FN      | TP  |"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python27]",
   "language": "python",
   "name": "conda-env-python27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
